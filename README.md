# PyTorch Autograd Tutorial

A minimal, educational implementation of PyTorch's automatic differentiation (autograd) system. This project demonstrates how computational graphs are built and how gradients flow backward through operations using the chain rule.


## Project Structure

### ðŸ“„ Files

#### 1. **[tensor.py](tensor.py)** - Core Tensor Class
The main `Tensor` class that wraps data and tracks gradients.

**Key Features:**
- Stores data as numpy arrays (supports scalars and vectors)
- Tracks whether gradients are needed (`requires_grad`)
- Links to backward operations via `grad_fn` (builds computational graph)
- Operator overloading (`__add__`, `__mul__`, `__pow__`) for natural syntax
- Accumulates gradients during backpropagation

**Example:**
```python
x = Tensor([1.0, 2.0, 3.0], requires_grad=True)
y = x ** 2  # Creates new tensor with grad_fn=PowBackward
y.backward() # Computes gradients
print(x.grad)  # [2. 4. 6.]
```

#### 2. **[backward_ops.py](backward_ops.py)** - Gradient Operations
Implements backward passes for each operation using the chain rule.

**Operations:**
- `AddBackward` - Addition: `âˆ‚z/âˆ‚a = 1, âˆ‚z/âˆ‚b = 1`
- `MulBackward` - Multiplication: `âˆ‚z/âˆ‚a = b, âˆ‚z/âˆ‚b = a`
- `PowBackward` - Power: `âˆ‚z/âˆ‚a = n Ã— a^(n-1)`

Each backward operation:
1. Receives gradient from the next layer (`grad_output`)
2. Computes local gradients using calculus rules
3. Passes gradients to inputs via chain rule

#### 3. **[demo.py](demo.py)** - Interactive Examples
Two comprehensive examples demonstrating scalar and vector operations with manual gradient verification.

---

## Demo Examples with Manual Computation

### Example 1: Scalar Tensors

**Computation:** `output = (x Ã— w1 + x Ã— w2 + b)Â²`

**Gradient Derivation:**

Let `temp = x Ã— w1 + x Ã— w2 + b`, then `output = tempÂ²`

**1. Apply Power Rule:**
```
output = tempÂ²
âˆ‚output/âˆ‚temp = 2 Ã— temp
```

**2. Apply Chain Rule to find âˆ‚output/âˆ‚x:**
```
temp = x Ã— w1 + x Ã— w2 + b
âˆ‚temp/âˆ‚x = w1 + w2

âˆ‚output/âˆ‚x = âˆ‚output/âˆ‚temp Ã— âˆ‚temp/âˆ‚x
           = 2 Ã— temp Ã— (w1 + w2)
           = 2(xw1 + xw2 + b)(w1 + w2)
```

**3. Apply Chain Rule to find âˆ‚output/âˆ‚w1:**
```
âˆ‚temp/âˆ‚w1 = x

âˆ‚output/âˆ‚w1 = âˆ‚output/âˆ‚temp Ã— âˆ‚temp/âˆ‚w1
            = 2 Ã— temp Ã— x
            = 2x(xw1 + xw2 + b)
```

**4. Apply Chain Rule to find âˆ‚output/âˆ‚w2:**
```
âˆ‚temp/âˆ‚w2 = x

âˆ‚output/âˆ‚w2 = âˆ‚output/âˆ‚temp Ã— âˆ‚temp/âˆ‚w2
            = 2 Ã— temp Ã— x
            = 2x(xw1 + xw2 + b)
```

**5. Apply Chain Rule to find âˆ‚output/âˆ‚b:**
```
âˆ‚temp/âˆ‚b = 1

âˆ‚output/âˆ‚b = âˆ‚output/âˆ‚temp Ã— âˆ‚temp/âˆ‚b
           = 2 Ã— temp Ã— 1
           = 2(xw1 + xw2 + b)
```

**Summary of Gradient Formulas:**
```
âˆ‚output/âˆ‚x  = 2(xw1 + xw2 + b)(w1 + w2)
âˆ‚output/âˆ‚w1 = 2x(xw1 + xw2 + b)
âˆ‚output/âˆ‚w2 = 2x(xw1 + xw2 + b)
âˆ‚output/âˆ‚b  = 2(xw1 + xw2 + b)
```

---

### Example 2: Vector Tensors (Element-wise)

**Computation:** `y = (x âŠ™ w + b)Â²` (âŠ™ denotes element-wise multiplication)

**Gradient Derivation (Element-wise):**

Let `temp = x âŠ™ w + b`, then `y = tempÂ²`

Since operations are element-wise, each component is independent. For component `i`:

**1. Power Rule (element-wise):**
```
y[i] = temp[i]Â²
âˆ‚y[i]/âˆ‚temp[i] = 2 Ã— temp[i]
```

**2. Chain Rule for âˆ‚y[i]/âˆ‚x[i]:**
```
temp[i] = x[i] Ã— w[i] + b[i]
âˆ‚temp[i]/âˆ‚x[i] = w[i]

âˆ‚y[i]/âˆ‚x[i] = âˆ‚y[i]/âˆ‚temp[i] Ã— âˆ‚temp[i]/âˆ‚x[i]
            = 2 Ã— temp[i] Ã— w[i]
            = 2w[i](x[i]w[i] + b[i])
```

**3. Chain Rule for âˆ‚y[i]/âˆ‚w[i]:**
```
âˆ‚temp[i]/âˆ‚w[i] = x[i]

âˆ‚y[i]/âˆ‚w[i] = âˆ‚y[i]/âˆ‚temp[i] Ã— âˆ‚temp[i]/âˆ‚w[i]
            = 2 Ã— temp[i] Ã— x[i]
            = 2x[i](x[i]w[i] + b[i])
```

**4. Chain Rule for âˆ‚y[i]/âˆ‚b[i]:**
```
âˆ‚temp[i]/âˆ‚b[i] = 1

âˆ‚y[i]/âˆ‚b[i] = âˆ‚y[i]/âˆ‚temp[i] Ã— âˆ‚temp[i]/âˆ‚b[i]
            = 2 Ã— temp[i] Ã— 1
            = 2(x[i]w[i] + b[i])
```

